{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise guides you through the process of classifying images using a Softmax classifier. As part of this you will:\n",
    "\n",
    "- Implement a fully vectorized loss function for the Softmax classifier\n",
    "- Calculate the analytical gradient using vectorized code\n",
    "- Tune hyperparameters on a validation set\n",
    "- Optimize the loss function with Stochastic Gradient Descent (SGD)\n",
    "- Visualize the learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# start-up code! \n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, validation and testing sets have been created as \n",
      " X_i and y_i where i=train,val,test\n",
      "Train data shape:  (3073, 49000)\n",
      "Train labels shape:  (49000,)\n",
      "Val data shape:  (3073, 1000)\n",
      "Val labels shape:  (1000,)\n",
      "Test data shape:  (3073, 1000)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from load_cifar10_tvt import load_cifar10_train_val\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10_train_val()\n",
    "print(\"Train data shape: \", X_train.shape)\n",
    "print(\"Train labels shape: \", y_train.shape)\n",
    "print(\"Val data shape: \", X_val.shape)\n",
    "print(\"Val labels shape: \", y_val.shape)\n",
    "print(\"Test data shape: \", X_test.shape)\n",
    "print(\"Test labels shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for this section is to be written in `cs231n/classifiers/softmax.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized loss: 2.397462e+00 computed in 0.296197s\n",
      "loss: 2.397462\n",
      "sanity check: 2.302585\n",
      "numerical: 0.610210 analytic: 29900.282873, relative error: 9.999592e-01\n",
      "numerical: 0.010884 analytic: 533.339877, relative error: 9.999592e-01\n",
      "numerical: 2.744904 analytic: 134500.296895, relative error: 9.999592e-01\n",
      "numerical: -1.329629 analytic: -65151.815441, relative error: 9.999592e-01\n",
      "numerical: 1.976073 analytic: 96827.573681, relative error: 9.999592e-01\n",
      "numerical: -1.399590 analytic: -68579.889621, relative error: 9.999592e-01\n",
      "numerical: 1.890660 analytic: 92642.318638, relative error: 9.999592e-01\n",
      "numerical: -1.119452 analytic: -54853.175156, relative error: 9.999592e-01\n",
      "numerical: 2.056254 analytic: 100756.426821, relative error: 9.999592e-01\n",
      "numerical: -5.783914 analytic: -283411.782888, relative error: 9.999592e-01\n"
     ]
    }
   ],
   "source": [
    "# Now, implement the vectorized version in softmax_loss_vectorized.\n",
    "\n",
    "import time\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "\n",
    "# gradient check.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "\n",
    "W = np.random.randn(10, 3073) * 0.0001\n",
    "\n",
    "tic = time.time()\n",
    "loss, grad = softmax_loss_vectorized(W, X_train, y_train, 0) #0.00001)\n",
    "toc = time.time()\n",
    "print(\"vectorized loss: %e computed in %fs\" % (loss, toc - tic))\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print(\"loss: %f\" % loss)\n",
    "print(\"sanity check: %f\" % (-np.log(0.1)))\n",
    "\n",
    "f = lambda w: softmax_loss_vectorized(w, X_train, y_train, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for this section is to be written in`cs231n/classifiers/linear_classifier.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now that efficient implementations to calculate loss function and gradient of the softmax are ready,\n",
    "# use it to train the classifier on the cifar-10 data\n",
    "# Complete the `train` function in cs231n/classifiers/linear_classifier.py\n",
    "\n",
    "from cs231n.classifiers.linear_classifier import Softmax\n",
    "\n",
    "classifier = Softmax()\n",
    "loss_hist = classifier.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    learning_rate=1e-3,\n",
    "    reg=1e-5,\n",
    "    num_iters=10,\n",
    "    batch_size=200,\n",
    "    verbose=False,\n",
    ")\n",
    "# Plot loss vs. iterations\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.ylabel(\"Loss value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Complete the `predict` function in cs231n/classifiers/linear_classifier.py\n",
    "# Evaluate on test set\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print(\"softmax on raw pixels final test set accuracy: %f\" % (test_accuracy,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = classifier.W[:, :-1]  # strip out the bias\n",
    "w = w.reshape(10, 32, 32, 3)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = [\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype(\"uint8\"))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
