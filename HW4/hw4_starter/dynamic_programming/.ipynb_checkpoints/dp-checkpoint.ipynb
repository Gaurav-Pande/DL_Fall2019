{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming (20 points + 10 bonus points)\n",
    "\n",
    "In this assignment, we will implement a few dynamic programming algorithms, namely, policy iteration and value iteration and run them on a simple MDP - the Frozen Lake environment.\n",
    "\n",
    "The sub-routines for these algorithms are present in `vi_and_pi.py` and must be filled out to test your implementation.\n",
    "\n",
    "The deliverables are located at the end of this notebook and show the point distrbution for each part. \n",
    "\n",
    "**Value iteration is worth 20 points of regular credit and policy iteration is worth 10 points of bonus credit for both sections of this course CS 7643 and CS 4803.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from lake_envs import *\n",
    "from vi_and_pi import *\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "env_d = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "env_s = gym.make(\"Stochastic-4x4-FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render Mode\n",
    "\n",
    "The variable `RENDER_ENV` is set `True` by default to allow you to see a rendering of the state of the environment at every time step. However, when you complete this assignment, you must set this to `False` and re-run all blocks of code. This is to prevent excessive amounts of rendered environments from being included in the final PDF.\n",
    "\n",
    "#### IMPORTANT: SET `RENDER_ENV` TO FALSE BEFORE SUBMISSION!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RENDER_ENV = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Value Iteration\n",
    "\n",
    "For the first part, you will implement the familiar value iteration update from class.\n",
    "\n",
    "In `vi_and_pi.pi` and complete the `value_iteration` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Use this space for debugging                    #\n",
    "# Make sure to delete this code before submission #\n",
    "###################################################\n",
    "pass\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to train value iteration and render a single episode of following the policy obtained at the end of value iteration. \n",
    "\n",
    "You should expect to get an Episode reward of `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "V_vi, p_vi = value_iteration(env_d.P, env_d.nS, env_d.nA, gamma=0.9, tol=1e-3)\n",
    "render_single(env_d, p_vi, 100, show_rendering=RENDER_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [BONUS] Part 2: Policy Iteration\n",
    "\n",
    "This is a bonus question in which you will implement policy iteration. If you do not wish to attempt this bonus quesiton, skip to the next part.\n",
    "\n",
    "In class, we studied the value iteration update:\n",
    "\n",
    "\n",
    "$$V_{t+1}(s) \\leftarrow \\max\\limits_a \\sum\\limits_{s'} p(s'|s, a)\\left[r(s, a) + \\gamma V_t(s')\\right]$$\n",
    "\n",
    "This is used to compute the value function $V^*$ corresponding to the optimal policy $\\pi^*$. We can alternatively compute the value function $V^\\pi$ corresponding to an arbitrary policy $\\pi$, with a similar update loop:\n",
    "\n",
    "$$V^\\pi_{t+1}(s) \\leftarrow \\sum_a \\pi(a|s) \\sum\\limits_{s'} p(s'|s, a)\\left[r(s, a) + \\gamma V^\\pi_t(s')\\right]$$\n",
    "\n",
    "On convergence, this will give us $V^\\pi$, which is the first step of a policy iteration update.\n",
    "\n",
    "The second step involves policy refinement, which will update the policy to take actions greedily with respect to $V^\\pi$:\n",
    "\n",
    "$$ \\pi_{new} \\leftarrow \\textrm{arg}\\max_a \\left[ r(s, a) + \\gamma \\sum_{s'} p(s'|s,a) V^\\pi(s') \\right] $$\n",
    "\n",
    "\n",
    "A single update of policy iteration involves the two above steps: (1) policy evaluation (which itself is an inner loop which will converge to $V^\\pi$ and (2) policy refinement. In the first part of assignment, you will implement the functions for policy evaluation, policy improvement (refinement) and policy iteration.\n",
    "\n",
    "In `vi_and_pi.pi` and complete the `policy_evaluation`, `policy_improvement` and `policy_iteration` functions. Run the blocks below to test your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Use this space for debugging                    #\n",
    "# Make sure to delete this code before submission #\n",
    "###################################################\n",
    "pass\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "V_pi, p_pi = policy_iteration(env_d.P, env_d.nS, env_d.nA, gamma=0.9, tol=1e-3)\n",
    "render_single(env_d, p_pi, 100, show_rendering=RENDER_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: VI on Stochastic Frozen Lake\n",
    "\n",
    "Now we will apply our implementation on an MDP where transitions to next states are stochastic. Modify your implementation of value iteration as needed so that policy iteration and value iteration work for stochastic transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Use this space for debugging                    #\n",
    "# Make sure to delete this code before submission #\n",
    "###################################################\n",
    "pass\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "V_vi, p_vi = value_iteration(env_s.P, env_s.nS, env_s.nA, gamma=0.9, tol=1e-3)\n",
    "render_single(env_s, p_vi, 100, show_rendering=RENDER_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [BONUS] Part 4: PI on Stochastic Frozen Lake\n",
    "\n",
    "This is a bonus question to run policy iteration on stochastic frozen lake.\n",
    "\n",
    "Now we will apply our implementation on an MDP where transitions to next states are stochastic. Modify your implementation of value iteration as needed so that policy iteration and value iteration work for stochastic transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Use this space for debugging                    #\n",
    "# Make sure to delete this code before submission #\n",
    "###################################################\n",
    "pass\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "V_pi, p_pi = policy_iteration(env_s.P, env_s.nS, env_s.nA, gamma=0.9, tol=1e-3)\n",
    "render_single(env_s, p_pi, 100, show_rendering=RENDER_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate All Policies\n",
    "\n",
    "Now, we will first test the value iteration implementation on two kinds of environments - the dererministic FrozenLake and the stochastic FrozenLake. We will also run the same for policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 1 (10 points)\n",
    "\n",
    "Run value iteration on deterministic FrozenLake. You should get a reward of 1.0 for full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nValue Iteration on Deterministic FrozenLake:\")\n",
    "V_vi, p_vi = value_iteration(env_d.P, env_d.nS, env_d.nA, gamma=0.9, tol=1e-3)\n",
    "evaluate(env_d, p_vi, max_steps=100, max_episodes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 2 (10 points)\n",
    "\n",
    "Run value iteration on stochastic FrozenLake. Note that this time, running the same policy over multiple episodes will result in different outcomes (final reward) due to stochastic transitions in the environment, and even the optimal policy may not succeed in reaching the goal state 100% of the time.\n",
    "\n",
    "You should get a reward of 0.7 or higher over 1000 episodes for full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValue Iteration on Stochastic FrozenLake:\")\n",
    "V_vi, p_vi = value_iteration(env_s.P, env_s.nS, env_s.nA, gamma=0.9, tol=1e-3)\n",
    "evaluate(env_s, p_vi, max_steps=100, max_episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 3 (5 bonus points)\n",
    "\n",
    "Run policy iteration on deterministic FrozenLake. You should get a reward of 1.0 for full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Policy Iteration on Deterministic FrozenLake:\")\n",
    "V_pi, p_pi = policy_iteration(env_d.P, env_d.nS, env_d.nA, gamma=0.9, tol=1e-3)\n",
    "evaluate(env_d, p_pi, max_steps=100, max_episodes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4 (5 bonus points)\n",
    "\n",
    "Run policy iteration on stochastic FrozenLake.\n",
    "\n",
    "You should get a reward of 0.7 or higher over 1000 episodes for full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Policy Iteration on Stochastic FrozenLake:\")\n",
    "V_pi, p_pi = policy_iteration(env_s.P, env_s.nS, env_s.nA, gamma=0.9, tol=1e-3)\n",
    "evaluate(env_s, p_pi, max_steps=100, max_episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Reminder\n",
    "\n",
    "#### PLEASE RE-RUN THE NOTEBOOK WITH `RENDER_ENV` SET TO FALSE BEFORE SUBMISSION!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
